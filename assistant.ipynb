{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "from pathlib import Path\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "# Whisper imports are in it's class initialization because of conditional dependencies\n",
    "\n",
    "# XTTS imports\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Ollama access imports\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whisper:\n",
    "    def __init__(self, using_as_package, model_name, model_path, min_duration, silence_threshold, silence_duration) -> None:\n",
    "        # Parameters\n",
    "        self.using_as_package = using_as_package\n",
    "        self.model_name = model_name\n",
    "        self.model_path = model_path\n",
    "        self.min_duration = min_duration\n",
    "        self.silence_threshold = silence_threshold\n",
    "        self.silence_duration = silence_duration\n",
    "\n",
    "        # Constants\n",
    "        self.SAMPLE_RATE = 16000\n",
    "        self.CHUNK_SIZE = 1024\n",
    "\n",
    "        # Load model\n",
    "        if self.using_as_package:\n",
    "            import whisper\n",
    "            self.model = whisper.load_model(self.model_name)\n",
    "        else:\n",
    "            from transformers import pipeline\n",
    "            device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "            self.pipe = pipeline(\n",
    "                \"automatic-speech-recognition\",\n",
    "                model=model_path,\n",
    "                chunk_length_s=30,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "    def record_until_silence(self):\n",
    "        \"\"\"Record audio until silence is detected.\"\"\"\n",
    "        recorded_chunks = []\n",
    "        silence_start_time = None\n",
    "        always_silent = True\n",
    "\n",
    "        def is_longer_than_min(recorded_chunks):\n",
    "            return len(recorded_chunks) > self.min_duration * self.SAMPLE_RATE / self.CHUNK_SIZE\n",
    "\n",
    "        def is_silent(chunk):\n",
    "            return np.abs(chunk).mean() < self.silence_threshold\n",
    "\n",
    "        # Start recording\n",
    "        with sd.InputStream(samplerate=self.SAMPLE_RATE, channels=1, dtype='float32', blocksize=self.CHUNK_SIZE) as stream:\n",
    "            while True:\n",
    "                # Read a chunk of audio data\n",
    "                chunk = stream.read(self.CHUNK_SIZE)[0]\n",
    "                recorded_chunks.append(chunk)\n",
    "\n",
    "                # Check for silence\n",
    "                if not is_silent(chunk):\n",
    "                    always_silent = False  # Reset always_silent flag if sound is detected\n",
    "\n",
    "                if is_longer_than_min(recorded_chunks) and is_silent(chunk):\n",
    "                    if silence_start_time is None:\n",
    "                        silence_start_time = time.time()  # Start the silence timer\n",
    "                    elif time.time() - silence_start_time > self.silence_duration:\n",
    "                        # print(\"Silence detected. Stopping recording.\")\n",
    "                        break  # Stop recording if silence is detected for the specified duration\n",
    "                else:\n",
    "                    silence_start_time = None  # Reset silence timer if sound is detected\n",
    "\n",
    "        # Concatenate all recorded chunks into a single array\n",
    "        audio_data = np.concatenate(recorded_chunks)\n",
    "        return audio_data, always_silent\n",
    "    \n",
    "    def transcript(self):\n",
    "        print(\"Recording...\")\n",
    "        \n",
    "        # Record audio until silence is detected\n",
    "        audio_data, always_silent = self.record_until_silence()\n",
    "\n",
    "        print(\"Recording complete.\")\n",
    "\n",
    "        if always_silent:\n",
    "            return None, always_silent\n",
    "\n",
    "        audio_data = audio_data.flatten()  # Ensure it's a 1D array\n",
    "\n",
    "        result = None\n",
    "        if self.using_as_package:\n",
    "            result = self.model.transcribe(audio_data)\n",
    "        else:\n",
    "            result = self.pipe(audio_data)\n",
    "        \n",
    "        return result[\"text\"], always_silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XTTS:\n",
    "    def __init__(self, speaker_wav, language) -> None:\n",
    "        # Parameters\n",
    "        self.speaker_wav = speaker_wav\n",
    "        self.language = language\n",
    "\n",
    "        # Constants\n",
    "        self.SAMPLE_RATE = 24000\n",
    "        \n",
    "        # Get device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Init TTS\n",
    "        self.tts = TTS(\"xtts_v2\").to(device)\n",
    "\n",
    "        self.infer = lambda text: self.tts.tts(text=text, speaker_wav=self.speaker_wav, language=self.language)\n",
    "\n",
    "        self.sd_thread = threading.Thread(target=lambda: None)\n",
    "        \n",
    "\n",
    "    def speak(self, text):\n",
    "        wav = self.infer(text)\n",
    "        \n",
    "        while self.sd_thread.is_alive():\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        if not self.sd_thread.is_alive():\n",
    "            sd.play(wav, samplerate=self.SAMPLE_RATE)\n",
    "            self.sd_thread = threading.Thread(target=sd.wait)\n",
    "            self.sd_thread.start()\n",
    "            # self.sd_thread = threading.Thread(target=sd.play, args=(wav, samplerate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ollama:\n",
    "    def __init__(self, url, model, xtts, temperature=0.3) -> None:\n",
    "        # Parameters\n",
    "        self.url = url\n",
    "        self.model = model\n",
    "        self.xtts = xtts\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def chat(self, messages):\n",
    "        r = requests.post(\n",
    "            self.url,\n",
    "            json={\"model\": self.model,\n",
    "                \"messages\": messages,\n",
    "                \"options\": {\"temperature\": self.temperature},\n",
    "                \"stream\": True},\n",
    "        stream=True\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "\n",
    "        output = \"\"\n",
    "        unfinished_sentence = \"\"\n",
    "        sentence = \"\"\n",
    "        \n",
    "        speak_thread = threading.Thread(target=lambda: None)\n",
    "\n",
    "        respond_iter = r.iter_lines()\n",
    "        # for line in r.iter_lines():\n",
    "        while True:\n",
    "            try:\n",
    "                body = json.loads(next(respond_iter))\n",
    "            except StopIteration:\n",
    "                pass\n",
    "            \n",
    "            if \"error\" in body:\n",
    "                raise Exception(body[\"error\"])\n",
    "            if body.get(\"done\") is False:\n",
    "                message = body.get(\"message\", \"\")\n",
    "                content = message.get(\"content\", \"\")\n",
    "                output += content\n",
    "                unfinished_sentence += content\n",
    "                # the response streams one token at a time, print that as we receive it\n",
    "                # print(content, end=\"\", flush=True)\n",
    "\n",
    "                # Wait for sentence-ending tokens and store into sentence\n",
    "                for word in unfinished_sentence:\n",
    "                    if word in ['.', '!', '?', '。', '！', '？'] and not word in ['...']:\n",
    "                        sentence += unfinished_sentence\n",
    "                        unfinished_sentence = \"\"\n",
    "\n",
    "            # Speak sentences with TTS\n",
    "            if not speak_thread.is_alive():\n",
    "                if len(sentence) > 0:\n",
    "                    print(sentence, flush=True)\n",
    "                    speak_thread = threading.Thread(target=self.xtts.speak, args=(sentence, ))\n",
    "                    speak_thread.start()\n",
    "                    sentence = \"\"\n",
    "                    continue\n",
    "                elif body.get(\"done\", False) and not self.xtts.sd_thread.is_alive():\n",
    "                    message[\"content\"] = output\n",
    "                    return message\n",
    "                else:\n",
    "                    time.sleep(0.1)\n",
    "            else:\n",
    "                time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 461M/461M [02:50<00:00, 2.84MiB/s]\n"
     ]
    }
   ],
   "source": [
    "whisper = Whisper(\n",
    "    using_as_package=True,  # TODO: Set to True if you are using Whisper as a package, False if you are using the Whisper model directly\n",
    "    model_name=\"small.en\",  # TODO: Update this to the name of the Whisper model to use if you are using Whisper as a package\n",
    "    model_path = \"path/to/your/model/whisper-small.en/\" ,  # TODO: update this to the path to the Whisper model if you are using the Whisper model directly\n",
    "    \n",
    "    min_duration = 3, # Duration in seconds at the beginning of the recording without silence detection\n",
    "    silence_threshold = 0.01,  # Threshold for silence detection\n",
    "    silence_duration = 1,  # Duration in seconds to consider as silence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n"
     ]
    }
   ],
   "source": [
    "xtts = XTTS(\n",
    "    speaker_wav = \"./voice_examples/edited_voice_example_lycaon_en.wav\",  # TODO: update this to the speaker's voice file\n",
    "    language = \"en\",  # TODO: update this to the language you wish to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(\n",
    "    url = \"http://localhost:11434/api/chat\",  # The URL of the Ollama API\n",
    "    model = \"llama3.1:8b\",  # TODO: update this to the model you wish to use\n",
    "    xtts = xtts,  # The XTTS object used to generate the audio\n",
    "    temperature = 0.5,  # The temperature of the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are a helpful assistant. Please respond in a conversational, speech-like style without any formatting.\n",
    "'''  # TODO: update this to your own prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete.\n",
      " Hello, hello. Why is the sky blue?\n",
      "The sky being blue is actually one of those really cool science-y things that's pretty easy to understand once you know what's going on.\n",
      "\n",
      "\n",
      " > Text splitted to sentences.\n",
      "[\"The sky being blue is actually one of those really cool science-y things that's pretty easy to understand once you know what's going on.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 10.303298234939575\n",
      " > Real-time factor: 0.9148400798934413\n",
      "So, basically, when sunlight enters Earth's atmosphere, it encounters all sorts of tiny molecules like nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      " > Text splitted to sentences.\n",
      "[\"So, basically, when sunlight enters Earth's atmosphere, it encounters all sorts of tiny molecules like nitrogen and oxygen.\", 'These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.']\n",
      " > Processing time: 19.825655698776245\n",
      " > Real-time factor: 0.7850764829571867\n",
      " That's why we see blue skies most of the time!\n",
      "\n",
      "It's kind of like when you're outside on a sunny day and you look up at the sky - it looks bright blue because all that scattered sunlight is bouncing around everywhere, hitting your eyes from every direction.\n",
      "\n",
      "Of course, there are some other factors that can affect how blue the sky appears, like pollution or dust in the air. But overall, it's just the way light behaves when it interacts with our atmosphere!\n",
      " > Text splitted to sentences.\n",
      "[\"That's why we see blue skies most of the time!\", \"It's kind of like when you're outside on a sunny day and you look up at the sky - it looks bright blue because all that scattered sunlight is bouncing around everywhere, hitting your eyes from every direction.\", 'Of course, there are some other factors that can affect how blue the sky appears, like pollution or dust in the air.', \"But overall, it's just the way light behaves when it interacts with our atmosphere!\"]\n",
      " > Processing time: 19.743542194366455\n",
      " > Real-time factor: 0.6626660761473763\n",
      "\n",
      "\n",
      "\n",
      "Recording...\n",
      "Recording complete.\n",
      " Thank you.\n",
      "It was my pleasure to help explain something as cool as why the sky is blue.\n",
      " > Text splitted to sentences.\n",
      "['It was my pleasure to help explain something as cool as why the sky is blue.']\n",
      " > Processing time: 3.938728094100952\n",
      " > Real-time factor: 0.7797815909614818\n",
      " If you have any more questions or want to learn about something else, feel free to ask me anytime!\n",
      " > Text splitted to sentences.\n",
      "['If you have any more questions or want to learn about something else, feel free to ask me anytime!']\n",
      " > Processing time: 5.358900547027588\n",
      " > Real-time factor: 0.675746620584902\n",
      "\n",
      "\n",
      "\n",
      "Recording...\n",
      "Recording complete.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    messages = []\n",
    "\n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    while True:\n",
    "        # user_input = input(\"Enter a prompt: \")\n",
    "        user_input, always_silent = whisper.transcript()\n",
    "        if always_silent or not user_input or user_input == \"\":\n",
    "            print(\"Nothing input. Pausing.\")\n",
    "            halt_input = input(\"Press Enter to resume or enter 'bye' to exit.\")\n",
    "            if halt_input == \"bye\":\n",
    "                bye = \"Exiting, goodbye.\"\n",
    "                print(bye)\n",
    "                xtts.speak(bye)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        print(user_input)\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        message = ollama.chat(messages)\n",
    "        messages.append(message)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
